****************简单易学的机器学习算法——集成方法(Ensemble Method)    https://blog.csdn.net/google19890102/article/details/46507387
**********************************************************************************
一、集成学习方法的思想
集成学习方法是指组合多个模型，以获得更好的效果，使集成的模型具有更强的泛化能力。对于多个模型，如何组合这些模型，主要有以下几种不同的方法：
      在验证数据集上找到表现最好的模型作为最终的预测模型；
      对多个模型的预测结果进行投票或者取平均值；
      对多个模型的预测结果做加权平均。

二、集成学习的主要方法
1、强可学习和弱可学习
   在集成学习方法中，是将多个弱模型，通过一定的组合方式，组合成一个强模型。

  强可学习——在概率近似正确(probably approximately correct, PAC)学习的框架中，一个概念(一个类)，
  如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称这个概念是。。。
  
  弱可学习——一个概念，如果存在一个多项式的学习算法能够学习它，学习正确率仅比随机猜测略好，那么就称这个概念是。。。

2、多个模型投票或者取平均值
   对于数据集训练多个模型，对于分类问题，可以采用投票的方法，选择票数最多的类别作为最终的类别，
   对于回归问题，可以采用取均值的方法，取得的均值作为最终的结果（采用简单的平均方法）
分类问题：采用投票的方法，得票最多的类别为最终的类别
回归问题：采用简单的平均方法
 
 在这样的思路里最著名的是Bagging方法.Bagging即Boostrap Aggregating，其中，Boostrap是一种有放回的抽样方法，其抽样策略是简单的随机抽样。
在Bagging方法中，让学习算法训练多次，每次的训练集由初始的训练集中随机取出的n个训练样本组成，初始的训练样本在某次的训练集中可能出现多次或者根本不出现。
最终训练出m个预测函数{h1,h2,...hm}，最终的预测函数为H
  随机森林算法就是基于Bagging思想的学习算法。

3、对多个模型的预测结果做加权平均
  在上述的Bagging方法中，其特点在于随机化抽样，通过反复的抽样训练新的模型，最终在这些模型的基础上取平均。
而在对多个模型的预测结果做加权平均则是将多个弱学习模型提升为强学习模型，这就是Boosting的核心思想。







几种集成预测模型及其应用 http://www.docin.com/p-919028232.html
